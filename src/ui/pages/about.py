import streamlit as st


def render():
    st.title("Sobre o FloraVision")
    
    st.markdown("""
    **Projeto FloraVision**: sistema de IA para detec√ß√£o de anomalias em folhas por 
    reconstru√ß√£o generativa e heatmaps, desenvolvido para a disciplina de 
    **Introdu√ß√£o √† Intelig√™ncia Artificial da UnB**.
    """)
    
    st.divider()
    
    # Overview section
    st.header("üåø Vis√£o Geral")
    st.markdown("""
    O FloraVision utiliza uma abordagem inovadora para detectar doen√ßas e anomalias 
    em folhas de plantas. Em vez de treinar um classificador tradicional que precisa 
    aprender padr√µes de cada doen√ßa espec√≠fica, nosso sistema aprende como uma folha 
    **saud√°vel** deve ser colorida ‚Äî e usa essa expectativa para identificar anomalias.
    
    A ideia central √© simples mas poderosa: se o modelo aprendeu perfeitamente como 
    colorir folhas saud√°veis, ele ter√° dificuldade em reconstruir corretamente as 
    cores de uma folha doente. Essa **diferen√ßa de reconstru√ß√£o** revela as anomalias.
    """)
    
    st.divider()
    
    # Pix2Pix section
    st.header("üß† A Arquitetura Pix2Pix")
    
    st.subheader("O que √© o Pix2Pix?")
    st.markdown("""
    O **Pix2Pix** √© uma Rede Adversarial Generativa Condicional (cGAN) desenvolvida 
    por Isola et al. (2017). Ele √© projetado para tarefas de **tradu√ß√£o de imagem 
    para imagem** ‚Äî onde queremos converter uma imagem de um dom√≠nio para outro 
    preservando a estrutura espacial.
    
    Exemplos cl√°ssicos incluem:
    - Converter mapas em fotos de sat√©lite
    - Colorir imagens em preto e branco
    - Converter esbo√ßos em fotos realistas
    - Transformar fotos diurnas em noturnas
    
    No nosso caso, usamos o Pix2Pix para **coloriza√ß√£o**: convertemos a vers√£o em 
    escala de cinza (canal L do espa√ßo LAB) para a vers√£o colorida (canais a e b).
    """)
    
    st.subheader("Componentes Principais")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üé® Gerador (U-Net)")
        st.markdown("""
        O gerador usa uma arquitetura **U-Net** com skip connections:
        
        - **Encoder**: Reduz progressivamente a resolu√ß√£o enquanto aumenta a 
          profundidade de features, capturando informa√ß√µes sem√¢nticas abstratas
        
        - **Decoder**: Reconstr√≥i a imagem na resolu√ß√£o original, usando as 
          features aprendidas
        
        - **Skip Connections**: Conectam camadas correspondentes do encoder e 
          decoder, preservando detalhes espaciais finos que seriam perdidos 
          durante a compress√£o
        
        Entrada: Canal L (lumin√¢ncia/escala de cinza)
        Sa√≠da: Canais a e b (informa√ß√£o de cor)
        """)
    
    with col2:
        st.markdown("### üîç Discriminador (PatchGAN)")
        st.markdown("""
        O discriminador usa uma arquitetura **PatchGAN**:
        
        - Em vez de classificar a imagem inteira como real ou falsa, avalia 
          **patches (regi√µes)** de 70x70 pixels
        
        - Isso for√ßa o gerador a produzir texturas realistas em toda a imagem
        
        - Reduz o n√∫mero de par√¢metros comparado a um discriminador tradicional
        
        - Durante o treinamento, tenta distinguir entre pares (input, output) 
          reais e gerados
        
        Entrada: Concatena√ß√£o do canal L + canais ab (real ou gerado)
        Sa√≠da: Mapa de probabilidades por patch
        """)
    
    st.subheader("Treinamento Adversarial")
    st.markdown("""
    O Pix2Pix √© treinado com duas fun√ß√µes de perda combinadas:
    
    1. **Perda Adversarial (GAN Loss)**: O gerador tenta enganar o discriminador, 
       enquanto o discriminador tenta distinguir imagens reais das geradas. Esse 
       "jogo" leva a resultados mais realistas.
    
    2. **Perda L1**: Penaliza a diferen√ßa pixel a pixel entre a sa√≠da gerada e a 
       imagem real. Isso garante que o gerador produza cores pr√≥ximas √†s originais, 
       n√£o apenas "realistas".
    
    A combina√ß√£o dessas perdas resulta em coloriza√ß√£o que √© tanto **visualmente 
    convincente** quanto **fiel √† imagem original**.
    """)
    
    st.divider()
    
    # Our approach section
    st.header("üî¨ Nossa Abordagem: Detec√ß√£o por Reconstru√ß√£o")
    
    st.subheader("Fase de Treinamento")
    st.markdown("""
    1. **Coleta de dados**: Reunimos imagens de folhas **apenas saud√°veis**
    
    2. **Pr√©-processamento**: Cada imagem √© convertida para o espa√ßo de cor LAB:
       - **L** (Lumin√¢ncia): Escala de cinza, de 0 a 100
       - **a**: Verde (-) a Vermelho (+)
       - **b**: Azul (-) a Amarelo (+)
    
    3. **Treinamento do modelo**: O Pix2Pix aprende a mapear L ‚Üí ab
       - Entrada: Canal L (estrutura da folha)
       - Sa√≠da: Canais ab (cores da folha)
       - Objetivo: Reconstruir perfeitamente as cores de folhas saud√°veis
    
    4. **Resultado**: Um modelo especialista em colorir folhas saud√°veis
    """)
    
    st.subheader("Fase de Detec√ß√£o")
    st.markdown("""
    1. **Nova imagem**: Recebemos uma folha para an√°lise (saud√°vel ou doente)
    
    2. **Extra√ß√£o**: Separamos o canal L (escala de cinza) da imagem
    
    3. **Coloriza√ß√£o**: O modelo gera os canais ab que ele "espera" ver
    
    4. **Compara√ß√£o**: Calculamos a diferen√ßa entre:
       - **Cor real**: Os canais ab originais da imagem
       - **Cor esperada**: Os canais ab gerados pelo modelo
    
    5. **M√©trica CIEDE2000**: Uma f√≥rmula perceptual que quantifica a diferen√ßa 
       de cor de forma similar √† vis√£o humana
    
    6. **Heatmap de anomalias**: Visualiza√ß√£o das regi√µes com maior diferen√ßa
    """)
    
    st.subheader("Por que funciona?")
    st.info("""
    **A intui√ß√£o √© elegante**: O modelo foi treinado exclusivamente com folhas 
    saud√°veis, ent√£o ele "conhece" apenas padr√µes de colora√ß√£o saud√°vel.
    
    Quando recebe uma folha doente:
    - O modelo tenta colorir como se fosse saud√°vel
    - As regi√µes doentes (manchas, necroses, descolora√ß√µes) ser√£o coloridas 
      "incorretamente"
    - A diferen√ßa entre a cor real (doente) e a cor esperada (saud√°vel) 
      revela a anomalia
    
    Isso √© chamado de **detec√ß√£o por anomalia de reconstru√ß√£o**.
    """)
    
    st.divider()
    
    # CIEDE2000 section
    st.header("üìä M√©trica CIEDE2000")
    st.markdown("""
    O **CIEDE2000** (ŒîE*‚ÇÄ‚ÇÄ) √© o padr√£o internacional para medir diferen√ßa 
    perceptual de cores. Desenvolvido pela CIE (Commission Internationale de 
    l'√âclairage), ele modela como humanos percebem diferen√ßas de cor.
    
    Caracter√≠sticas importantes:
    
    | Valor ŒîE | Percep√ß√£o |
    |----------|-----------|
    | < 1.0 | Impercept√≠vel |
    | 1.0 - 2.0 | Percept√≠vel apenas por olhos treinados |
    | 2.0 - 3.5 | Percept√≠vel |
    | 3.5 - 5.0 | Diferen√ßa clara |
    | > 5.0 | Cores claramente diferentes |
    
    No contexto do FloraVision:
    - **Score baixo** (< 3.0): A coloriza√ß√£o foi precisa ‚Üí folha provavelmente saud√°vel
    - **Score alto** (> 3.0): A coloriza√ß√£o divergiu ‚Üí poss√≠vel anomalia
    """)
    
    st.divider()
    
    # Heatmaps section
    st.header("üó∫Ô∏è Heatmaps e Visualiza√ß√£o")
    st.markdown("""
    O sistema gera **mapas de calor (heatmaps)** que mostram pixel a pixel onde 
    as anomalias est√£o localizadas:
    
    - **Cores frias (azul/verde)**: Baixa diferen√ßa de cor ‚Üí Regi√£o saud√°vel
    - **Cores quentes (amarelo/vermelho)**: Alta diferen√ßa de cor ‚Üí Poss√≠vel anomalia
    
    Isso permite:
    - **Localiza√ß√£o precisa** das regi√µes afetadas
    - **Quantifica√ß√£o** da severidade da anomalia
    - **Verifica√ß√£o visual** para valida√ß√£o humana
    """)
    
    st.subheader("Grad-CAM")
    st.markdown("""
    Al√©m dos heatmaps de diferen√ßa de cor, oferecemos visualiza√ß√£o **Grad-CAM** 
    (Gradient-weighted Class Activation Mapping):
    
    - Mostra quais regi√µes da imagem a rede neural considera mais importantes
    - Ajuda a entender o "racioc√≠nio" do modelo
    - √ötil para validar se o modelo est√° focando nas regi√µes corretas
    """)
    
    st.divider()
    
    # Technical details
    st.header("‚öôÔ∏è Detalhes T√©cnicos")
    
    with st.expander("Arquitetura do Gerador (U-Net 256)"):
        st.code("""
Encoder (downsampling):
  Conv2d(1, 64) ‚Üí LeakyReLU
  Conv2d(64, 128) ‚Üí BatchNorm ‚Üí LeakyReLU
  Conv2d(128, 256) ‚Üí BatchNorm ‚Üí LeakyReLU
  Conv2d(256, 512) ‚Üí BatchNorm ‚Üí LeakyReLU
  Conv2d(512, 512) ‚Üí BatchNorm ‚Üí LeakyReLU
  Conv2d(512, 512) ‚Üí BatchNorm ‚Üí LeakyReLU
  Conv2d(512, 512) ‚Üí BatchNorm ‚Üí LeakyReLU
  Conv2d(512, 512) ‚Üí ReLU (bottleneck)

Decoder (upsampling com skip connections):
  ConvTranspose2d(512, 512) ‚Üí BatchNorm ‚Üí Dropout ‚Üí ReLU + skip
  ConvTranspose2d(1024, 512) ‚Üí BatchNorm ‚Üí Dropout ‚Üí ReLU + skip
  ConvTranspose2d(1024, 512) ‚Üí BatchNorm ‚Üí Dropout ‚Üí ReLU + skip
  ConvTranspose2d(1024, 512) ‚Üí BatchNorm ‚Üí ReLU + skip
  ConvTranspose2d(1024, 256) ‚Üí BatchNorm ‚Üí ReLU + skip
  ConvTranspose2d(512, 128) ‚Üí BatchNorm ‚Üí ReLU + skip
  ConvTranspose2d(256, 64) ‚Üí BatchNorm ‚Üí ReLU + skip
  ConvTranspose2d(128, 2) ‚Üí Tanh

Entrada: 1 x 256 x 256 (canal L normalizado)
Sa√≠da: 2 x 256 x 256 (canais ab normalizados)
        """, language="text")
    
    with st.expander("Espa√ßo de Cor LAB"):
        st.markdown("""
        O espa√ßo de cor **CIELAB** (LAB) separa luminosidade de informa√ß√£o de cor:
        
        - **L** (Luminosidade): 0 (preto) a 100 (branco)
        - **a**: -128 (verde) a +127 (vermelho)  
        - **b**: -128 (azul) a +127 (amarelo)
        
        Vantagens para coloriza√ß√£o:
        1. **Separa√ß√£o de estrutura e cor**: O canal L cont√©m toda a informa√ß√£o 
           de bordas, texturas e formas
        2. **Perceptualmente uniforme**: Dist√¢ncias no espa√ßo LAB correspondem 
           melhor √† percep√ß√£o humana
        3. **Natural para a tarefa**: O modelo aprende apenas a adicionar cor, 
           n√£o a modificar a estrutura
        """)
    
    with st.expander("Par√¢metros de Treinamento"):
        st.markdown("""
        Configura√ß√µes t√≠picas para o treinamento:
        
        | Par√¢metro | Valor | Descri√ß√£o |
        |-----------|-------|-----------|
        | `netG` | unet_256 | Arquitetura do gerador |
        | `ngf` | 64 | Filtros base do gerador |
        | `n_epochs` | 100 | √âpocas com LR constante |
        | `n_epochs_decay` | 100 | √âpocas com LR decrescente |
        | `lr` | 0.0002 | Learning rate inicial |
        | `beta1` | 0.5 | Par√¢metro Adam |
        | `lambda_L1` | 100 | Peso da perda L1 |
        | `batch_size` | 4-16 | Depende da GPU dispon√≠vel |
        """)
    
    st.divider()
    
    # References
    st.header("üìö Refer√™ncias")
    st.markdown("""
    1. **Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A.** (2017). 
       *Image-to-Image Translation with Conditional Adversarial Networks*. 
       CVPR 2017. [arXiv:1611.07004](https://arxiv.org/abs/1611.07004)
    
    2. **Zhu, J. Y., Park, T., Isola, P., & Efros, A. A.** (2017). 
       *Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks*. 
       ICCV 2017. [arXiv:1703.10593](https://arxiv.org/abs/1703.10593)

    3. **Ryoya Katafuchi, Terumasa Tokunaga (2020)**
        *Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors*
        [arXiv:2011.14306](https://arxiv.org/abs/2011.14306)
    """)
    
